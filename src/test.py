import os
import sys
import torch

# è§£å†³ OMP æŠ¥é”™çš„å…³é”®ï¼šå…è®¸é‡å¤åŠ è½½åº“
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from model import TransformerEncoder

    print("âœ… æˆåŠŸæ‰¾åˆ° TransformerEncoder ç±»ï¼")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit()


def test():
    # ä½¿ç”¨ä½œä¸šè¦æ±‚çš„è¶…å‚æ•° [cite: 80]
    vocab_size = 1000
    d_model = 128
    n_layers = 2
    h = 4

    model = TransformerEncoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, h=h)

    # ç»Ÿè®¡æ¨¡å‹å‚æ•° (è¿›é˜¶åŠ åˆ†é¡¹ )
    total_params = model.count_parameters()
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")

    # æ¨¡æ‹Ÿè¾“å…¥ (batch_size=32, seq_len=64) [cite: 80]
    dummy_input = torch.randint(0, vocab_size, (32, 64))

    # å‰å‘ä¼ æ’­
    output = model(dummy_input)

    print(f"è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")

    if output.shape == (32, 64, vocab_size):
        print("ğŸ‰ ç»´åº¦æ ¡éªŒæˆåŠŸï¼å‡†å¤‡è¿›å…¥è®­ç»ƒé˜¶æ®µã€‚")


if __name__ == "__main__":
    test()