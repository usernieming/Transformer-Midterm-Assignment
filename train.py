import os

# å¿…é¡»åœ¨ import torch ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import sys

# ç¡®ä¿èƒ½å¯¼å…¥ src é‡Œçš„æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from src.model import TransformerEncoder
from src.dataset import TinyShakespeareDataset

# ç¡¬ä»¶æ£€æµ‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}")

# æŒ‰ç…§ä½œä¸šè¡¨ 3 è®¾ç½®è¶…å‚æ•° [cite: 79, 80]
LR = 3e-4
BATCH_SIZE = 32
SEQ_LEN = 64
EPOCHS = 1000

# åˆå§‹åŒ–æ•°æ®å’Œæ¨¡å‹ [cite: 10]
ds = TinyShakespeareDataset(seq_len=SEQ_LEN)
model = TransformerEncoder(
    vocab_size=ds.vocab_size,
    d_model=128,
    n_layers=2,
    h=4
).to(device)

print(f"æ¨¡å‹æ€»å‚æ•°é‡: {model.count_parameters():,}")

# è¿›é˜¶æŠ€å·§ï¼šAdamW ä¼˜åŒ–å™¨ [cite: 19]
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss()
loss_history = []

print("ğŸš€ æ­£åœ¨å¯åŠ¨è®­ç»ƒ...")
model.train()
for i in range(EPOCHS):
    x, y = ds.get_batch(BATCH_SIZE)
    x, y = x.to(device), y.to(device)

    logits = model(x)
    loss = criterion(logits.view(-1, ds.vocab_size), y.view(-1))

    optimizer.zero_grad()
    loss.backward()

    # è¿›é˜¶æŠ€å·§ï¼šæ¢¯åº¦è£å‰ª [cite: 19]
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    loss_history.append(loss.item())

    if i % 100 == 0:
        print(f"è¿­ä»£ {i:4d} | Loss: {loss.item():.4f}")

# ç¡®ä¿ results æ–‡ä»¶å¤¹å­˜åœ¨å¹¶ä¿å­˜æ›²çº¿ [cite: 15, 19]
os.makedirs("results", exist_ok=True)
plt.figure(figsize=(10, 5))
plt.plot(loss_history)
plt.title("Training Loss Curve")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.grid(True)
plt.savefig("results/loss_curve.png")

# ä¿å­˜æ¨¡å‹ [cite: 19]
# ä¿®æ”¹åçš„ä»£ç ï¼šæ‰‹åŠ¨åŒºåˆ†æ–‡ä»¶å
save_name = "loss_no_pos"  # å½“ä½ æ³¨é‡Šæ‰ä½ç½®ç¼–ç æ—¶ï¼ŒæŠŠè¿™é‡Œæ”¹å†™æˆè¿™ä¸ªåå­—
# save_name = "loss_with_pos" # å½“ä½ æœ‰ä½ç½®ç¼–ç æ—¶ï¼Œç”¨è¿™ä¸ªåå­—

plt.savefig(f"results/{save_name}.png")
torch.save(model.state_dict(), f"results/{save_name}.pth")
print(f"âœ… è®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ results/{save_name}.png")